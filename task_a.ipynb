{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3766cc-ed5e-4149-8152-7ab4f408ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a50126-236f-4459-90c4-7e3519738703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load the webis-touche2020 dataset\n",
    "dataset = ir_datasets.load(\"beir/webis-touche2020/v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875d1c3-14d0-45c3-9b69-733580e48f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to store the vocabulary terms\n",
    "vocabulary_terms = set()\n",
    "\n",
    "initial_tokens_count = 0\n",
    "\n",
    "# initial tokens before preprocessing\n",
    "for doc in dataset.docs_iter():\n",
    "    initial_tokens_count += len(doc.text.split())\n",
    "\n",
    "print(f\"Initial Tokens Count: {initial_tokens_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41609b0e-3db0-4ab4-a0b7-fe4701369b3c",
   "metadata": {},
   "source": [
    "## Define preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fe0071-e3cf-474a-aa8c-0b2bd8927e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_non_alphanum(text):\n",
    "    # Remove non-alphanumeric characters\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "def text_lowercase(text):\n",
    "    # Convert text to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Remove punctuation\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    # Remove extra whitespaces\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Remove stopwords using NLTK's English stopwords list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def stem_words(text):\n",
    "    # Stem words using NLTK's Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def spell_correction(text):\n",
    "    # Correct spelling using the SpellChecker library\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word is not None:\n",
    "            corrected_words.append(corrected_word)\n",
    "        else:\n",
    "            # If the correction is None, keep the original word\n",
    "            corrected_words.append(word)\n",
    "    \n",
    "    return ' '.join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21d6bc-bf53-4574-969c-049a11043152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a set to store the vocabulary terms\n",
    "vocabulary_terms = set()\n",
    "\n",
    "# Apply each preprocessing step \n",
    "for preprocessing_function, title in [ \n",
    "                                       (text_lowercase, \"Text Lowercase\"),\n",
    "                                       (remove_non_alphanum, \"Remove Non-Alphanumeric\"), \n",
    "                                       (remove_punctuation, \"Remove Punctuation\"), \n",
    "                                       (remove_whitespace, \"Remove Whitespace\"), \n",
    "                                       (remove_stopwords, \"Remove Stopwords\"), \n",
    "                                       (stem_words, \"Stem Words\"), \n",
    "                                       (spell_correction, \"Spell Correction\")\n",
    "                                       ]:\n",
    "    # reset set\n",
    "    vocabulary_terms = set()\n",
    "\n",
    "    # apply particular preprocessing to all docs\n",
    "    \n",
    "    for doc in dataset.docs_iter():\n",
    "        preprocessed_text = doc.text\n",
    "        preprocessed_text = preprocessing_function(preprocessed_text)\n",
    "        \n",
    "        # Update the vocabulary terms with unique terms in the preprocessed text\n",
    "        vocabulary_terms.update(preprocessed_text.split())\n",
    "    \n",
    "    # Print the vocabulary size at this stage\n",
    "    print(f\"{title} - Vocabulary Size: {len(vocabulary_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1406fd8-162e-45f6-b0eb-90d26182be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final vocabulary size after all preprocessing steps\n",
    "final_vocabulary_size = len(vocabulary_terms)\n",
    "print(f\"Final Vocabulary Size: {final_vocabulary_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
