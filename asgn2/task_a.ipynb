{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "from collections import Counter\n",
    "import ir_datasets\n",
    "import nltk\n",
    "\n",
    "# Load the final vocabulary\n",
    "with open('final_vocabulary.pkl', 'rb') as file:\n",
    "    loaded_vocabulary = pickle.load(file)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ir_datasets.load(\"beir/webis-touche2020/v2\")\n",
    "total_documents = dataset.docs_count()\n",
    "\n",
    "def calculate_df(term):\n",
    "    # Initialize the document frequency\n",
    "    df = 0\n",
    "\n",
    "    # Iterate over the documents in the dataset\n",
    "    for doc in dataset.docs_iter():\n",
    "        # Convert the document into a list of terms\n",
    "        document_terms = doc.text.split()\n",
    "\n",
    "        # If the term is in the document, increment the document frequency\n",
    "        if term in document_terms:\n",
    "            df += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_ranking(sample_sentence):\n",
    "    # List of weighting schemes\n",
    "    weighting_schemes = ['standard', 'log_max', 'double_norm_prob']\n",
    "\n",
    "    # Iterate through each weighting scheme\n",
    "    for weighting_scheme in weighting_schemes:\n",
    "        print(f\"Weighting Scheme: {weighting_scheme}\\n\")\n",
    "\n",
    "        # Create an empty dictionary to store the TF-IDF scores for the sample sentence\n",
    "        tfidf_scores = {}\n",
    "\n",
    "        # Tokenize the sample sentence\n",
    "        terms = sample_sentence.split()\n",
    "\n",
    "        # Compute TF-IDF for each term in the sample sentence\n",
    "        for term in terms:\n",
    "            if term in loaded_vocabulary:\n",
    "                term_frequency = terms.count(term)\n",
    "                document_frequency = calculate_df(term)\n",
    "                inverse_document_frequency = math.log(total_documents / (1 + document_frequency))\n",
    "\n",
    "                if weighting_scheme == 'standard':\n",
    "                    tfidf = term_frequency * inverse_document_frequency\n",
    "\n",
    "                elif weighting_scheme == 'log_max':\n",
    "                    tfidf = (1 + math.log(term_frequency)) * inverse_document_frequency\n",
    "\n",
    "                elif weighting_scheme == 'double_norm_prob':\n",
    "                    max_term_frequency = max(terms.count(t) for t in terms)\n",
    "                    tfidf = (0.5 + 0.5 * term_frequency / max_term_frequency) * inverse_document_frequency\n",
    "\n",
    "                tfidf_scores[term] = tfidf\n",
    "\n",
    "        # Rank terms based on their TF-IDF scores\n",
    "        sorted_terms = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Print the top N ranked terms\n",
    "        N = 10\n",
    "        for rank, (term, score) in enumerate(sorted_terms[:N], start=1):\n",
    "            print(f\"Rank {rank}: Term '{term}', TF-IDF Score: {score}\")\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382545\n"
     ]
    }
   ],
   "source": [
    "print(total_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Doc:  The resolution used by Pro *assumes* that Australia isn't already a 'significant' country - however, in actual reality, it is. Firstly we should clarify what significance means: 1.a the state or quality of being significant1.b of consequence or importance==================================To respond directly to Pros argument first:he/she asserts that Australia invented 'amazing things' like 'WiFI, Google Maps, Polymer bank notes, Ultrasound scanners, stainless steel braces and many more things'.  ...\n",
      "\n",
      "Query:  Should social security be privatized? \n",
      "\n",
      "Weighting Scheme: standard\n",
      "\n",
      "Rank 1: Term 'social', TF-IDF Score: 3.1401581224063317\n",
      "Rank 2: Term 'be', TF-IDF Score: 0.5594517683977795\n",
      "\n",
      "\n",
      "Weighting Scheme: log_max\n",
      "\n",
      "Rank 1: Term 'social', TF-IDF Score: 3.1401581224063317\n",
      "Rank 2: Term 'be', TF-IDF Score: 0.5594517683977795\n",
      "\n",
      "\n",
      "Weighting Scheme: double_norm_prob\n",
      "\n",
      "Rank 1: Term 'social', TF-IDF Score: 3.1401581224063317\n",
      "Rank 2: Term 'be', TF-IDF Score: 0.5594517683977795\n",
      "\n",
      "\n",
      "-------------------\n",
      "Doc:  First of all we invented amazing things like WiFi, Google Maps, Polymer bank notes (if you are American and do not know what they are, they are plastic WATERPROOF bills), Ultrasound scanners, stainless steel braces and many more things. Why put us into the shadow if we have made such amazing things the whole world uses nowadays! I bet you have used at the very least ONE thing I put up there unless you are on a Ethernet cable still using those old paper maps wherever you go! There is no point in  ...\n",
      "\n",
      "Query:  Is a college education worth it? \n",
      "\n",
      "Weighting Scheme: standard\n",
      "\n",
      "Rank 1: Term 'college', TF-IDF Score: 4.392075782313749\n",
      "Rank 2: Term 'worth', TF-IDF Score: 3.7555280128400828\n",
      "Rank 3: Term 'education', TF-IDF Score: 3.6995625161965853\n",
      "Rank 4: Term 'a', TF-IDF Score: 0.3601071280707242\n",
      "\n",
      "\n",
      "Weighting Scheme: log_max\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the documents and queries in the dataset\n",
    "ctr = 0\n",
    "\n",
    "for doc, query in zip(dataset.docs_iter(), dataset.queries_iter()):\n",
    "    # print the doc text and query text\n",
    "    if ctr < 4:\n",
    "        ctr += 1\n",
    "        continue\n",
    "\n",
    "    print('-------------------')\n",
    "    # print the first 400 characters of the doc text\n",
    "    print(\"Doc: \", doc.text[:500], \"...\\n\")\n",
    "    print(\"Query: \", query.text, \"\\n\")\n",
    "\n",
    "    get_ranking(query.text)\n",
    "\n",
    "    ctr += 1\n",
    "\n",
    "    if ctr == 4:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
