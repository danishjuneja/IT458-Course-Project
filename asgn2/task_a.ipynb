{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\codei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\codei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "from collections import Counter\n",
    "import ir_datasets\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the final vocabulary\n",
    "with open('final_vocabulary.pkl', 'rb') as file:\n",
    "    loaded_vocabulary = pickle.load(file)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ir_datasets.load(\"beir/webis-touche2020/v2\")\n",
    "total_documents = dataset.docs_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_doc(query):\n",
    "    query = text_lowercase(query)\n",
    "    query = remove_non_alpha(query)\n",
    "    query = remove_num(query)\n",
    "    query = remove_punc(query)\n",
    "    query = remove_whitespace(query)\n",
    "    query = remove_stopwords(query)\n",
    "\n",
    "    return query\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_non_alpha(text):\n",
    "    res = \"\"\n",
    "    for elem in text:\n",
    "        if elem.isalnum() or elem == \" \":\n",
    "          res += elem\n",
    "    return res\n",
    "\n",
    "def remove_num(text):\n",
    "    res = re.sub(r'\\d+', '', text)\n",
    "    return res\n",
    "\n",
    "def remove_punc(text):\n",
    "    trans = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(trans)\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_df(term):\n",
    "    # Initialize the document frequency\n",
    "    df = 0\n",
    "\n",
    "    # Iterate over the documents in the dataset\n",
    "    for doc in dataset.docs_iter():\n",
    "        # Convert the document into a list of terms\n",
    "        document_terms = doc.text.split()\n",
    "\n",
    "        # If the term is in the document, increment the document frequency\n",
    "        if term in document_terms:\n",
    "            df += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_tfidf(sentence, weighting_scheme):\n",
    "    # Create an empty dictionary to store the TF-IDF scores for the sample sentence\n",
    "    tfidf_scores = {}\n",
    "\n",
    "    # Tokenize the sample sentence\n",
    "    terms = pre_process_doc(sentence)\n",
    "\n",
    "        # Compute TF-IDF for each term in the sample sentence\n",
    "    for term in terms:\n",
    "        if term in loaded_vocabulary:\n",
    "            term_frequency = terms.count(term)\n",
    "            document_frequency = calculate_df(term)\n",
    "            inverse_document_frequency = math.log(total_documents / (1 + document_frequency))\n",
    "\n",
    "            if weighting_scheme == 'standard':\n",
    "                tfidf = term_frequency * inverse_document_frequency\n",
    "            elif weighting_scheme == 'log_max':\n",
    "                    tfidf = (1 + math.log(term_frequency)) * inverse_document_frequency\n",
    "\n",
    "            elif weighting_scheme == 'double_norm_prob':\n",
    "                    max_term_frequency = max(terms.count(t) for t in terms)\n",
    "                    tfidf = (0.5 + 0.5 * term_frequency / max_term_frequency) * inverse_document_frequency\n",
    "        tfidf_scores[term] += tfidf\n",
    "        return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Query:  Should teachers get tenure? \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\codei\\Desktop\\NITK\\eighthsem\\Information Retrieval\\IT458-Course-Project\\asgn2\\task_a.ipynb Cell 4\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m scheme \u001b[39min\u001b[39;00m weighting_schemes:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mdocs_iter():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         doc_vec \u001b[39m=\u001b[39m get_tfidf(doc\u001b[39m.\u001b[39;49mtext, scheme)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         query_vec \u001b[39m=\u001b[39m get_tfidf(query\u001b[39m.\u001b[39mtext, scheme)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m cos_sim \u001b[39m=\u001b[39m cosine_similarity(doc_vec\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), query_vec\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "\u001b[1;32mc:\\Users\\codei\\Desktop\\NITK\\eighthsem\\Information Retrieval\\IT458-Course-Project\\asgn2\\task_a.ipynb Cell 4\u001b[0m line \u001b[0;36mget_tfidf\u001b[1;34m(sentence, weighting_scheme)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m term \u001b[39min\u001b[39;00m loaded_vocabulary:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     term_frequency \u001b[39m=\u001b[39m terms\u001b[39m.\u001b[39mcount(term)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     document_frequency \u001b[39m=\u001b[39m calculate_df(term)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     inverse_document_frequency \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mlog(total_documents \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m document_frequency))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mif\u001b[39;00m weighting_scheme \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstandard\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\codei\\Desktop\\NITK\\eighthsem\\Information Retrieval\\IT458-Course-Project\\asgn2\\task_a.ipynb Cell 4\u001b[0m line \u001b[0;36mcalculate_df\u001b[1;34m(term)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Iterate over the documents in the dataset\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mdocs_iter():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Convert the document into a list of terms\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     document_terms \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39msplit()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/codei/Desktop/NITK/eighthsem/Information%20Retrieval/IT458-Course-Project/asgn2/task_a.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# If the term is in the document, increment the document frequency\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ir_datasets\\indices\\lz4_pickle.py:64\u001b[0m, in \u001b[0;36mLz4PickleIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbin\u001b[39m.\u001b[39mseek(new_pos) \u001b[39m# this seek is smart -- if alrady in buffer, skips to that point\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice\u001b[39m.\u001b[39mstart\n\u001b[1;32m---> 64\u001b[0m result \u001b[39m=\u001b[39m _read_next(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbin, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlookup\u001b[39m.\u001b[39;49m_doc_cls)\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice\u001b[39m.\u001b[39mstep \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice\u001b[39m.\u001b[39mstop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice\u001b[39m.\u001b[39mstep)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ir_datasets\\indices\\lz4_pickle.py:19\u001b[0m, in \u001b[0;36m_read_next\u001b[1;34m(f, data_cls)\u001b[0m\n\u001b[0;32m     17\u001b[0m lz4 \u001b[39m=\u001b[39m ir_datasets\u001b[39m.\u001b[39mlazy_libs\u001b[39m.\u001b[39mlz4_block()\n\u001b[0;32m     18\u001b[0m content_length \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m\u001b[39m.\u001b[39mfrom_bytes(f\u001b[39m.\u001b[39mread(\u001b[39m4\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mlittle\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m content \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread(content_length)\n\u001b[0;32m     20\u001b[0m content \u001b[39m=\u001b[39m lz4\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mdecompress(content)\n\u001b[0;32m     21\u001b[0m content \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mloads(content)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "result = []\n",
    "for query in dataset.queries_iter():\n",
    "    print('-------------------')\n",
    "    print(\"Query: \", query.text, \"\\n\")\n",
    "    weighting_schemes = ['standard', 'log_max', 'double_norm_prob']\n",
    "    for scheme in weighting_schemes:\n",
    "        for doc in dataset.docs_iter():\n",
    "            doc_vec = get_tfidf(doc.text, scheme)\n",
    "            query_vec = get_tfidf(query.text, scheme)\n",
    "\n",
    "    cos_sim = cosine_similarity(doc_vec.reshape(1,-1), query_vec.reshape(1,-1))\n",
    "    result.append([doc,cos_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sorted(result, key=lambda x:x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res2 = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"doc {}:\".format(i+1) + res2[0][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
