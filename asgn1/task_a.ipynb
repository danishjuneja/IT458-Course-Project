{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3766cc-ed5e-4149-8152-7ab4f408ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a50126-236f-4459-90c4-7e3519738703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import pickle\n",
    "\n",
    "#for multi threading\n",
    "from multiprocessing import Pool\n",
    "num_cores = 15\n",
    "\n",
    "# Load the webis-touche2020 dataset\n",
    "dataset = ir_datasets.load(\"beir/webis-touche2020/v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f875d1c3-14d0-45c3-9b69-733580e48f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Tokens Count: 109582718\n"
     ]
    }
   ],
   "source": [
    "# set to store the vocabulary terms\n",
    "vocabulary_terms = set()\n",
    "\n",
    "initial_tokens_count = 0\n",
    "\n",
    "# initial tokens before preprocessing\n",
    "for doc in dataset.docs_iter():\n",
    "    initial_tokens_count += len(doc.text.split())\n",
    "\n",
    "print(f\"Initial Tokens Count: {initial_tokens_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41609b0e-3db0-4ab4-a0b7-fe4701369b3c",
   "metadata": {},
   "source": [
    "### Define preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00fe0071-e3cf-474a-aa8c-0b2bd8927e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_non_alphanum(text):\n",
    "    # Remove non-alphanumeric characters\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "def text_lowercase(text):\n",
    "    # Convert text to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Remove punctuation\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    # Remove extra whitespaces\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Remove stopwords using NLTK's English stopwords list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def stem_words(text):\n",
    "    # Stem words using NLTK's Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def spell_correction(text):\n",
    "    # Correct spelling using the SpellChecker library (Not used as resource heavy\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word is not None:\n",
    "            corrected_words.append(corrected_word)\n",
    "        else:\n",
    "            # If the correction is None, keep the original word. small fix\n",
    "            corrected_words.append(word)\n",
    "    \n",
    "    return ' '.join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10463442",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_functions = [\n",
    "    (text_lowercase, \"Text Lowercase\"),\n",
    "    (remove_whitespace, \"Remove Whitespace\"), \n",
    "    (remove_non_alphanum, \"Remove Non-Alphanumeric\"), \n",
    "    (remove_punctuation, \"Remove Punctuation\"), \n",
    "    (remove_stopwords, \"Remove Stopwords\"), \n",
    "    (stem_words, \"Stem Words\"),\n",
    "    (text_lowercase, \"Text Lowercase\"),# rerunning to make sure everything is lower case :) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f21d6bc-bf53-4574-969c-049a11043152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Lowercase - Vocabulary Size: 1810550\n",
      "Remove Whitespace - Vocabulary Size: 1810550\n",
      "Remove Non-Alphanumeric - Vocabulary Size: 891063\n",
      "Remove Punctuation - Vocabulary Size: 891063\n",
      "Remove Stopwords - Vocabulary Size: 890914\n",
      "Stem Words - Vocabulary Size: 786790\n",
      "Text Lowercase - Vocabulary Size: 786790\n"
     ]
    }
   ],
   "source": [
    "# Initialize a set to store the vocabulary terms\n",
    "vocabulary_terms = set()\n",
    "\n",
    "# Function to apply a specific preprocessing step to a single document\n",
    "def apply_preprocessing_step(args):\n",
    "    doc_id, doc_text, preprocessing_function = args\n",
    "    preprocessed_text = preprocessing_function(doc_text)\n",
    "    return doc_id, preprocessed_text\n",
    "\n",
    "#initialize data for preprocessing\n",
    "preprocessed_data = [(doc.doc_id, doc.text) for doc in dataset.docs_iter()]\n",
    "\n",
    "with Pool(num_cores) as pool:\n",
    "    for preprocessing_function, title in preprocessing_functions:\n",
    "        # Apply the current preprocessing step to the preprocessed data in parallel\n",
    "        preprocessed_data = pool.map(apply_preprocessing_step, [(doc_id, doc_text, preprocessing_function) for doc_id, doc_text in preprocessed_data])\n",
    "        \n",
    "        # Calculate and print the vocabulary size at this stage\n",
    "        vocabulary_terms = set()\n",
    "        vocabulary_terms.update((' '.join(text for _, text in preprocessed_data)).split())\n",
    "        print(f\"{title} - Vocabulary Size: {len(vocabulary_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1406fd8-162e-45f6-b0eb-90d26182be5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary(token) size:109582718\n",
      "Final Vocabulary Size: 786790\n",
      "Percentage Reduction in Vocabulary Size: 99.28%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print the final vocabulary size after all preprocessing steps\n",
    "final_vocabulary_size = len(vocabulary_terms)\n",
    "print(f\"Initial Vocabulary(token) size:{initial_tokens_count}\")\n",
    "print(f\"Final Vocabulary Size: {final_vocabulary_size}\")\n",
    "\n",
    "percentage_reduction = ((initial_tokens_count - final_vocabulary_size) / initial_tokens_count) * 100\n",
    "print(f\"Percentage Reduction in Vocabulary Size: {percentage_reduction:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79be7b7d",
   "metadata": {},
   "source": [
    "Store the optimized Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12270f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final vocabulary and preprocessed data to files using pickle\n",
    "with open('final_vocabulary.pkl', 'wb') as file:\n",
    "    pickle.dump(vocabulary_terms, file)\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as file:\n",
    "    pickle.dump(preprocessed_data, file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
