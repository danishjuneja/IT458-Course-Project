{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be3766cc-ed5e-4149-8152-7ab4f408ef1b",
   "metadata": {},
   "source": [
    "## Import Required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a50126-236f-4459-90c4-7e3519738703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be7b7d",
   "metadata": {},
   "source": [
    "load the optimized Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12270f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.pkl', 'rb') as file:\n",
    "    loaded_vocabulary = pickle.load(file)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ir_datasets.load(\"beir/webis-touche2020/v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2204af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess the text data\n",
    "documents = [query.text for query in dataset.queries_iter()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a92f538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1375: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create a vocabulary of unigrams and bigrams using the loaded vocabulary\n",
    "# You can adjust the parameters of CountVectorizer as needed\n",
    "vectorizer_unigram = CountVectorizer(ngram_range=(1, 1), vocabulary=loaded_vocabulary)\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(1, 2), vocabulary=loaded_vocabulary)\n",
    "\n",
    "# Fit and transform the documents to create BoW representations\n",
    "X_unigram = vectorizer_unigram.fit_transform(documents)\n",
    "X_bigram = vectorizer_bigram.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35c330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate the BoW representation for each document\n",
    "\n",
    "# Get the vocabulary for unigrams and bigrams\n",
    "vocab_unigram = vectorizer_unigram.get_feature_names_out()\n",
    "vocab_bigram = vectorizer_bigram.get_feature_names_out()\n",
    "\n",
    "# Convert BoW representations to dense arrays for analysis\n",
    "X_unigram_dense = X_unigram.toarray()\n",
    "X_bigram_dense = X_bigram.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1cce15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity for Unigram BoW: 0.9999975718057509\n",
      "Sparsity for Bigram BoW: 0.9999975718057509\n",
      "\n",
      "Top 10 most common bigrams:\n",
      "should: 33\n",
      "be: 25\n",
      "is: 10\n",
      "the: 10\n",
      "to: 8\n",
      "for: 6\n",
      "legal: 5\n",
      "in: 5\n",
      "have: 3\n",
      "do: 3\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Compare and analyze the two corpus representations\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity_unigram = 1 - (np.count_nonzero(X_unigram_dense) / np.prod(X_unigram_dense.shape))\n",
    "sparsity_bigram = 1 - (np.count_nonzero(X_bigram_dense) / np.prod(X_bigram_dense.shape))\n",
    "\n",
    "# Print sparsity results\n",
    "print(\"Sparsity for Unigram BoW:\", sparsity_unigram)\n",
    "print(\"Sparsity for Bigram BoW:\", sparsity_bigram)\n",
    "\n",
    "# Analyze spatial context (co-occurrence) for unigrams and bigrams\n",
    "# Find the most common bigrams\n",
    "bigram_counts = np.sum(X_bigram_dense, axis=0)\n",
    "most_common_bigrams = [(vocab_bigram[i], bigram_counts[i]) for i in np.argsort(bigram_counts)[::-1][:10]]\n",
    "\n",
    "print(\"\\nTop 10 most common bigrams:\")\n",
    "for bigram, count in most_common_bigrams:\n",
    "    print(f\"{bigram}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
